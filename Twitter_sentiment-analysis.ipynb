{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0b0e3e-cc40-4c67-b417-d4d976ac3d6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#                    NLP assignment 2 \n",
    "##                       2291588"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5183db5-512a-41ea-9173-981437deaf22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define test sets (edit this one to put in new test set 4 and 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9093490f-240e-4103-a7e5-8a4337278f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']\n",
    "# testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt', 'twitter-test4.txt', 'twitter-test5.txt']\n",
    "# I repeated two of test sets to make sure load more works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247a7bc-2c69-47bb-9364-bc1fe4dbf375",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Details of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514612d-2486-498a-ae01-3fe4249adda3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load libraries Run all of this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "176da4fd-2ef5-456c-b8f9-2b82af98dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library preprocess\n",
    "import re\n",
    "import emoji\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "from os.path import join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7197d898-25d0-4a20-814d-3c6ba54c29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library Part 2\n",
    "import pandas as pd\n",
    "# bow tfidf ngram \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#word2vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "# normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "# resampler\n",
    " # imbalanced data \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# for testing best resample method\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import ADASYN\n",
    "# hyper parameter\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "# bayesian\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# svc\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e698996c-9ac2-44cd-b6a2-24a0b8c1dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# decay learning rate\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from matplotlib import pyplot as plt\n",
    "#get word index\n",
    "import collections\n",
    "# for attention\n",
    "import torch.nn.functional as F\n",
    "# for evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "# for he initialization\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "29431aef-f534-4c6c-a8a0-b2ee606172c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a function to read data.\n",
    "def read_twitter_data(data_path):\n",
    "    list_name=[]\n",
    "    file = open(data_path, 'r', encoding='utf-8')\n",
    "    \n",
    "    for line in file.readlines():\n",
    "        dic = line\n",
    "        list_name.append(dic)\n",
    "    return list_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fe1bce64-3cbc-47dc-b6be-cc6dc3451329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train data\n",
    "train_data=read_twitter_data(\"twitter-training-data.txt\")\n",
    "# read dev data\n",
    "dev_data=read_twitter_data(\"twitter-dev-data.txt\")\n",
    "# read test 1 data\n",
    "test_data1=read_twitter_data(\"twitter-test1.txt\")\n",
    "# read test 2 data\n",
    "test_data2=read_twitter_data(\"twitter-test2.txt\")\n",
    "#read test 3 data\n",
    "test_data3=read_twitter_data(\"twitter-test3.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc256c6b-9a72-4530-9c32-bd7a1a034929",
   "metadata": {
    "tags": []
   },
   "source": [
    "Abbreviation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "80e71697-257d-4c60-ae90-e9ba89bdf112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  abbreviation dictionary cited from Kaggle There are many better dictionaries but this one works well for sentiment analysis\n",
    "# Twitter Sentiment Analysis with Naive Bayes 85%acc\n",
    "# https://www.kaggle.com/code/lykin22/twitter-sentiment-analysis-with-naive-bayes-85-acc\n",
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"â‚¬\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", \n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "     \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15038c42-629b-432d-9ba8-990a0b91fc46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "602315d2-a3ae-4b83-ad74-3197fc5f8d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_item(raw_data):\n",
    "    split_list=[]\n",
    "    for raw_data in raw_data:\n",
    "        # Split the input string by '\\t'\n",
    "        split_str = raw_data.split('\\t')\n",
    "        split_list.append(split_str)\n",
    "    return split_list\n",
    "# cite from kaggle deal with abb\n",
    "def convert_abb(tw):\n",
    "    t=[]\n",
    "    words=tw.split()\n",
    "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n",
    "    return ' '.join(t)\n",
    "def preprocess(text):\n",
    "    # Common preprocess\n",
    "    # delete stop_word but keep negation words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # negation_words = ['no', 'not']\n",
    "    negation_words = ['no', 'not', \"aren't\", 'but',\n",
    "                     'neither', 'nor', \"ain't\", \"can't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\",\n",
    "                     \"isn't\", \"weren't\", \"won't\", \"wouldn't\", \"shouldn't\", \"mustn't\"]\n",
    "    stop_words = stop_words - set(negation_words)\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)  \n",
    "    # remove usernames\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d+', '', text) # remove digits\n",
    "   \n",
    "    # transfer abbrevations into words\n",
    "    text=convert_abb(text)# translate abb\n",
    "    # translate emoji\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \")) \n",
    "    #remove _ in emoji\n",
    "    text = re.sub(r'_', r' ', text)\n",
    "    # remove words with hastags\n",
    "    text = re.sub(r'\\s*#\\S+', '', text)\n",
    "    # remove punctuations and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    # tokenize and convert to lowercase\n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove words with length less than or equal to 2 ï¼Œ as these word are not relevant to meaning except no\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if len(word) > 2 or word=='no' ]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    processed_twitter=text = ' '.join(lemmatized_tokens)\n",
    "    return processed_twitter\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eed859-8d5e-431b-8cdb-1184d2ca048f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2 Data scaling; deal with Imbalance data \n",
    "### Run all codes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c4661165-741f-44b1-a64f-261ed0f3d062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data and make data ready for vectorization\n",
    "def split_data(data1):\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    dic_X=[]\n",
    "    for i in range(len(data1)):\n",
    "        X_train.append(data1[i][2])\n",
    "        Y_train.append(data1[i][1])\n",
    "        dic_X.append(data1[i][0])\n",
    "    return X_train,Y_train,dic_X\n",
    "# we only need to preprocess the content \n",
    "def preprocess_data(input_data):\n",
    "    dici=[]\n",
    "    data1=split_item(input_data)\n",
    "    for i in range(0,len(data1)):\n",
    "        dici.append(preprocess(data1[i][2]))\n",
    "    X,Y,D=split_data(data1)\n",
    "    return dici,Y,D\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "544c6368-9735-4f5d-a9e1-61aeebce36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get id; sentiment and content for each data set\n",
    "Xtrain_1,Ytrain,D=preprocess_data(train_data)\n",
    "Xdev_1,Ydev,d0=preprocess_data(dev_data)\n",
    "Xtest1_1,Ytest1,d1=preprocess_data(test_data1)\n",
    "Xtest2_1,Ytest2,d2=preprocess_data(test_data2)\n",
    "Xtest3_1,Ytest3,d3=preprocess_data(test_data3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7ecc21dc-e307-49f4-a2bc-40c9471f8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other data\n",
    "# Load training set, dev set and testing set\n",
    "# I did not load train and dev data here as they have different use, test sets will only be used for test\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweets = {}\n",
    "# Note the tweets[dataset] is preprocessed tweets\n",
    "for dataset in  testsets:\n",
    "    \n",
    "    data[dataset] = read_twitter_data(dataset)\n",
    "    Xt,Yt,Dt=preprocess_data(data[dataset])\n",
    "    tweets[dataset]=Xt\n",
    "    tweetids[dataset] = Dt\n",
    "    tweetgts[dataset] = Yt\n",
    "\n",
    "    # write code to read in the datasets here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbc0cc-6442-4d85-a795-013cee0103cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test process \n",
    "code in this part is to test all features scaling and resampling method. It was turned to raw and no need to be run. Only change them to code if you want to test all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7180030-fa1d-48bf-9e85-923ec419614a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test step 1 Pick one of Four types of feature ( change raw to code for all step )\n",
    "### Only Pick one of them!  no need to run if only want to see best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94197c0-0339-4fe5-82d7-4e988269a152",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bag of word 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94a79caf-9ba7-4f0e-91b2-4d1a805ad019",
   "metadata": {},
   "source": [
    "# Bag of word 1\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_resample = bow_vectorizer.fit_transform(Xtrain_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cfa1818-9c87-480d-8e87-7c2a06db877b",
   "metadata": {},
   "source": [
    "# vectorize Xtrain \n",
    "Xtrain=X_train_resample\n",
    "#dev\n",
    "Xdev=bow_vectorizer.transform(Xdev_1)\n",
    "# test1\n",
    "Xtest1=bow_vectorizer.transform(Xtest1_1)\n",
    "# test2\n",
    "Xtest2=bow_vectorizer.transform(Xtest2_1)\n",
    "# test3\n",
    "Xtest3=bow_vectorizer.transform(Xtest3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934ef10-0e00-45f7-aa88-56e365124b65",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tfidf 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "939ecf8a-275b-4b8c-b903-6641a13d67d2",
   "metadata": {},
   "source": [
    "# Tfidf Vectorizer\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "X_train_resample = tf_vectorizer.fit_transform(Xtrain_1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d8da3aa-7d3b-4f7e-89ea-41fe4707dff4",
   "metadata": {},
   "source": [
    "# vectorize Xtrain \n",
    "Xtrain=X_train_resample\n",
    "#dev\n",
    "Xdev=tf_vectorizer.transform(Xdev_1)\n",
    "# test1\n",
    "Xtest1=tf_vectorizer.transform(Xtest1_1)\n",
    "# test2\n",
    "Xtest2=tf_vectorizer.transform(Xtest2_1)\n",
    "# test3\n",
    "Xtest3=tf_vectorizer.transform(Xtest3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e64b8-4fc2-4bbc-86af-db9339e3c88f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ngram"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87dbe1ea-6e81-4c66-9e33-d66df44fb373",
   "metadata": {},
   "source": [
    "\n",
    "# 1-3 Ngram\n",
    "n_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# change to vector\n",
    "X_train_resample = n_vectorizer.fit_transform(Xtrain_1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9f84037-cee9-4d03-b79b-c927ee0cd391",
   "metadata": {},
   "source": [
    "\n",
    "# vectorize Xtrain \n",
    "Xtrain=X_train_resample\n",
    "#dev\n",
    "Xdev=n_vectorizer.transform(Xdev_1)\n",
    "# test1\n",
    "Xtest1=n_vectorizer.transform(Xtest1_1)\n",
    "# test2\n",
    "Xtest2=n_vectorizer.transform(Xtest2_1)\n",
    "# test3\n",
    "Xtest3=n_vectorizer.transform(Xtest3_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65b30a-36e0-4634-afde-9e7e45feb71d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "611e7e37-7615-4ef7-8033-05f8f8aa2b73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# word2 vec\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "# create tokenized objects\n",
    "sentences = [x.split() for x in Xtrain_1]\n",
    "\n",
    "# Create a Word2Vec model using the tokenized objects\n",
    "modelw = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "modelw.train(sentences, total_examples=modelw.corpus_count, epochs=30)\n",
    "\n",
    "# Create the feature vectors\n",
    "X = []\n",
    "for tweet in sentences:\n",
    "    vectors = [modelw.wv[word] for word in tweet if word in modelw.wv.index_to_key]\n",
    "    if len(vectors) > 0:\n",
    "        X.append(sum(vectors) / len(vectors))\n",
    "    else:\n",
    "        X.append([0] * 100) # handle cases where there are no valid words\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98cb8665-53e0-491d-a742-4f75f072a802",
   "metadata": {
    "tags": []
   },
   "source": [
    "def get_word2vec(Xtest):\n",
    "    test_sentences = [tweet.split() for tweet in Xtest]\n",
    "    test_X = []\n",
    "    for tweet in test_sentences:\n",
    "        vectors = [modelw.wv[word] for word in tweet if word in modelw.wv.index_to_key]\n",
    "        if len(vectors) > 0:\n",
    "            test_X.append(sum(vectors) / len(vectors))\n",
    "        else:\n",
    "            test_X.append([0] * 100) \n",
    "    return test_X\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83bef931-de73-4bc0-a36d-e200cdbf48f7",
   "metadata": {},
   "source": [
    "X_train_resample=X\n",
    "\n",
    "Xtrain=X_train_resample\n",
    "Xdev=get_word2vec(Xdev_1)\n",
    "Xtest1=get_word2vec(Xtest1_1)\n",
    "Xtest2=get_word2vec(Xtest2_1)\n",
    "Xtest3=get_word2vec(Xtest3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9bebb-d169-4e88-81fd-a883b9df6c7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### All out put data in this part are ready for putting in classifier Three approach has the same variable name for comvinence, so only run one of the three codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221d662-eef1-4d6b-b35d-14484a72df06",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test step 2 Four types of imbalanced data process + original data\n",
    " All data in this part are stored in list with 5 resample methods including original data Run all codes here"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f78373c0-6f9a-4d7d-8b29-8ed29d363a7f",
   "metadata": {},
   "source": [
    " # imbalanced data \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# for testing best resample method\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f8a2437-b857-4b10-bf86-94d7e35233bc",
   "metadata": {},
   "source": [
    "# over sampling 2\n",
    "\n",
    "# Perform over-sampling using RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_train_resampled2, Y_train_resampled2 = ros.fit_resample(X_train_resample, Ytrain)\n",
    "\n",
    "# Perform under sampling using random undersampler 3\n",
    "\n",
    "undersample=RandomUnderSampler()\n",
    "X_train_resampled3, Y_train_resampled3 = undersample.fit_resample(X_train_resample, Ytrain)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4933a32d-bd8b-4391-827e-6a56d259371c",
   "metadata": {},
   "source": [
    "resampled_data=[[X_train_resampled2, Y_train_resampled2],\n",
    "                [X_train_resampled3, Y_train_resampled3]]\n",
    "Name_list=['oversample','undersample']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8e20f-678a-4019-bc0f-fb4f3c21f478",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Changed to mark down as not used later use this to replace previous resampled data if you want to try all resample method "
   ]
  },
  {
   "cell_type": "raw",
   "id": "25669664-bfd1-4aa3-bcff-1e225f44b1d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "sm = SMOTE()\n",
    "X_train_resampled1, Y_train_resampled1 = sm.fit_resample(X_train_resample, Ytrain)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c83305f4-b050-4918-be6c-d3d97443e335",
   "metadata": {},
   "source": [
    "\n",
    "tomek = TomekLinks()\n",
    "\n",
    "X_train_resampled4, Y_train_resampled4 = tomek.fit_resample(X_train_resample, Ytrain)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af6849a2-abb7-4194-88d2-1501487a4fac",
   "metadata": {},
   "source": [
    "\n",
    "resampled_data=[[Xtrain,Ytrain],[X_train_resampled1, Y_train_resampled1],\n",
    "                [X_train_resampled2, Y_train_resampled2],\n",
    "                [X_train_resampled3, Y_train_resampled3],\n",
    "                [X_train_resampled4, Y_train_resampled4]]\n",
    "Name_list=['original','SMOTE','oversample','undersample','Tomeklinks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281144fe-3935-4696-b0cc-8dd6361acf59",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Test step 3 pick one of Data scaling \n",
    " All codes here are different kind of data scaling with the same variable name, please only run one of them.(after change to code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b667ec-555d-4a0a-8039-ab8c7fc9c62d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalized"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3e16f26-c05f-43f6-a17a-48ae6cb09778",
   "metadata": {},
   "source": [
    "\n",
    "normalized_data=[]\n",
    "for i in range(0,len(resampled_data)):\n",
    "    normali=normalize(resampled_data[i][0])\n",
    "    normalized_data.append([normali,resampled_data[i][1]])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdd96489-4a1b-4d5f-a975-bb683b402ac3",
   "metadata": {},
   "source": [
    "# Xn = normalize(Xtrain, norm='l2')\n",
    "Xdevn=normalize(Xdev,norm='l2')\n",
    "Xtest1n=normalize(Xtest1, norm='l2')\n",
    "Xtest2n=normalize(Xtest2, norm='l2')\n",
    "Xtest3n=normalize(Xtest3, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b39f2-705a-4aea-8c7d-810d3cc0b6cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Max ABS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2dac8e8-d50b-47f8-8696-248a7dbf0692",
   "metadata": {},
   "source": [
    "from sklearn import preprocessing\n",
    "# called this because don't want to change the name\n",
    "normalized_data=[]\n",
    "for i in range(0,len(resampled_data)):\n",
    "    maxa = preprocessing.MaxAbsScaler().fit(resampled_data[i][0])\n",
    "    X_maxa = maxa.transform(resampled_data[i][0])\n",
    "    normalized_data.append([X_maxa,resampled_data[i][1]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dab089f8-5882-426c-a7f4-7d07393a1357",
   "metadata": {},
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler().fit(Xtrain)\n",
    "\n",
    "Xn = max_abs_scaler.transform(Xtrain)\n",
    "Xdevn=max_abs_scaler.transform(Xdev)\n",
    "Xtest1n=max_abs_scaler.transform(Xtest1)\n",
    "Xtest2n=max_abs_scaler.transform(Xtest2)\n",
    "Xtest3n=max_abs_scaler.transform(Xtest3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14e4b8-7b7b-45a7-9187-05319307e842",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standarization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f616b08d-32d9-49a9-a37a-bcf41c600c05",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "normalized_data=[]\n",
    "for i in range(0,len(resampled_data)):\n",
    "    scale = preprocessing.StandardScaler(with_mean=False).fit(resampled_data[i][0])\n",
    "    X_s = scale.transform(resampled_data[i][0])\n",
    "    normalized_data.append([X_s,resampled_data[i][1]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58b16faa-a784-4a51-86c3-c97151c6281b",
   "metadata": {},
   "source": [
    "scaler = preprocessing.StandardScaler(with_mean=False).fit(Xtrain)\n",
    "Xn = scaler.transform(Xtrain)\n",
    "Xdevn=scaler.transform(Xdev)\n",
    "Xtest1n=scaler.transform(Xtest1)\n",
    "Xtest2n=scaler.transform(Xtest2)\n",
    "Xtest3n=scaler.transform(Xtest3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ced587-98fd-43c7-8e53-e61eff9c08c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Min max"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2817679-581d-4dbe-ae66-36f7562deca9",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "normalized_data=[]\n",
    "for i in range(0,len(resampled_data)):\n",
    "    maxa = preprocessing.MinMaxScaler().fit(resampled_data[i][0])\n",
    "    X_maxa = maxa.transform(resampled_data[i][0])\n",
    "    normalized_data.append([X_maxa,resampled_data[i][1]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0c2cafe-32ae-438c-b12b-93c259f1cbb7",
   "metadata": {},
   "source": [
    "maxmin=preprocessing.MinMaxScaler().fit(Xtrain)\n",
    "Xn = maxmin.transform(Xtrain)\n",
    "Xdevn=maxmin.transform(Xdev)\n",
    "Xtest1n=maxmin.transform(Xtest1)\n",
    "Xtest2n=maxmin.transform(Xtest2)\n",
    "Xtest3n=maxmin.transform(Xtest3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94b344-c041-4bea-bdc8-3d55cfc4cfcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation code given code must run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e3143ed5-1a4a-4c25-9f70-060b0f7caf1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f25be-51b4-4673-a947-a3416c1cad44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayesian classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b11ca30-76d6-4bdf-836f-4b3b7378dfec",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  test step 4 run each classifier's test process"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c0acf3d-f491-482e-9eab-733587e8f955",
   "metadata": {
    "tags": []
   },
   "source": [
    "change to markdown\n",
    "# The best combination is  undersample Normalizer and BOW\n",
    "# find bayesian paramter\n",
    "\n",
    "param_grid = {'alpha': [ 1.0, 10.0,100],\n",
    "              'fit_prior': [True, False],\n",
    "              'class_prior': [None, [0.1, 0.2,0.7], [0.35, 0.2,0.45]]}\n",
    "\n",
    "\n",
    "bayes = MultinomialNB()\n",
    "\n",
    "# Use GridSearchCV to perform 5-fold cross-validation and search for the best hyperparameters \n",
    "# Only use use averageprecision scoring, \n",
    "grid_search = GridSearchCV(bayes, param_grid, cv=5,scoring='f1_macro')\n",
    "\n",
    "grid_search.fit(normalized_data[1][0], normalized_data[1][1])\n",
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_b = grid_search.best_params_\n",
    "best_scoreb=grid_search.best_score_\n",
    "print(\"Best parameter is:\", best_params_b)\n",
    "print('Best macro f1 is:',best_scoreb)\n",
    "# Best parameter is: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n",
    "# Best macro f1 is: 0.702149511394855  overfit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fc742-36b5-4b8e-9a8e-84755c7e7b62",
   "metadata": {
    "tags": []
   },
   "source": [
    " Train bayesian to run change back to code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee07937a-d81a-4564-856a-de7258bf1805",
   "metadata": {},
   "source": [
    "\n",
    "for j in range (0,len(normalized_data)):\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(normalized_data[j][0],normalized_data[j][1] )\n",
    "    \n",
    "    \n",
    "    print(Name_list[j])\n",
    "    dict0={}\n",
    "    y_p0=nb_classifier.predict(Xdevn)\n",
    "    for i in range(0,len(y_p0)):\n",
    "        dict0[d0[i]]=y_p0[i]\n",
    "    r0=evaluate(dict0,'twitter-dev-data.txt','nb_classifier')\n",
    "    dict1={}\n",
    "    y_p1= nb_classifier.predict(Xtest1n)\n",
    "    for i in range(0,len(y_p1)):\n",
    "        dict1[d1[i]]=y_p1[i]\n",
    "    dict2={}\n",
    "    y_p2= nb_classifier.predict(Xtest2n)\n",
    "    for i in range(0,len(y_p2)):\n",
    "        dict2[d2[i]]=y_p2[i]\n",
    "    dict3={}\n",
    "    y_p3= nb_classifier.predict(Xtest3n)\n",
    "    for i in range(0,len(y_p3)):\n",
    "        dict3[d3[i]]=y_p3[i]\n",
    "    r1=evaluate(dict1,'twitter-test1.txt','nb_classifier')\n",
    "    r2=evaluate(dict2,'twitter-test2.txt','nb_classifier')\n",
    "    r3=evaluate(dict3,'twitter-test3.txt','nb_classifier')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323ff1b-fa3d-4d92-9435-8da269f46813",
   "metadata": {
    "tags": []
   },
   "source": [
    "### best bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c1ae0bfb-ac37-4f8b-b3d5-4942041152c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bayesian train data and test data preprocessed\n",
    "# 1-3 Ngram\n",
    "nb_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "# change to ngram vector\n",
    "X_train_bayesian = nb_vectorizer.fit_transform(Xtrain_1)\n",
    "# resample \n",
    "ros = RandomOverSampler()\n",
    "Xtrain_br, Ytrain_br = ros.fit_resample(X_train_bayesian, Ytrain)\n",
    "Xtrainb = normalize(Xtrain_br, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "36fb410a-5860-415a-a106-f321f9a0bb01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bayesian classifier on test set\n",
      "twitter-test1.txt (nb_classifier): 0.594\n",
      "            positive  negative  neutral\n",
      "positive    0.578     0.070     0.351     \n",
      "negative    0.202     0.535     0.263     \n",
      "neutral     0.208     0.133     0.659     \n",
      "\n",
      "Results for Bayesian classifier on test set\n",
      "twitter-test2.txt (nb_classifier): 0.622\n",
      "            positive  negative  neutral\n",
      "positive    0.658     0.059     0.283     \n",
      "negative    0.159     0.534     0.307     \n",
      "neutral     0.271     0.083     0.646     \n",
      "\n",
      "Results for Bayesian classifier on test set\n",
      "twitter-test3.txt (nb_classifier): 0.557\n",
      "            positive  negative  neutral\n",
      "positive    0.628     0.070     0.302     \n",
      "negative    0.247     0.397     0.356     \n",
      "neutral     0.261     0.119     0.620     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output for test data\n",
    "# ngram features for test set\n",
    "nb_ngram_feature={}\n",
    "for datasets in testsets:\n",
    "    nb_ngram_feature[datasets]=normalize(nb_vectorizer.transform(tweets[datasets]),norm='l2')\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(Xtrainb,Ytrain_br )\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=nb_classifier.predict(nb_ngram_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for Bayesian classifier on test set')\n",
    "    evaluate(dict_output,item,'nb_classifier')\n",
    "    confusion(dict_output,item,'nb_classifier')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa1472-aeaa-4ff7-a43d-7654f679eb26",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SVM classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7aae8-8786-4751-aa6a-65b307150a1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4 test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4017d74b-e0b1-4742-a64e-4f814e5b0b0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train  SVM classifier\n",
    "for i in range (0,len(resampled_data)):\n",
    "    svm_classifier = LinearSVC()\n",
    "    svm_classifier.fit(normalized_data[i][0],normalized_data[i][1] )\n",
    "    \n",
    "    print(Name_list[i])\n",
    "    dict0={}\n",
    "    y_p0=svm_classifier.predict(Xdevn)\n",
    "    for i in range(0,len(y_p0)):\n",
    "        dict0[d0[i]]=y_p0[i]\n",
    "    r0=evaluate(dict0,'twitter-dev-data.txt','svm')\n",
    "    dict1={}\n",
    "    y_p1=svm_classifier.predict(Xtest1n)\n",
    "    for i in range(0,len(y_p1)):\n",
    "        dict1[d1[i]]=y_p1[i]\n",
    "    dict2={}\n",
    "    y_p2=svm_classifier.predict(Xtest2n)\n",
    "    for i in range(0,len(y_p2)):\n",
    "        dict2[d2[i]]=y_p2[i]\n",
    "    dict3={}\n",
    "    y_p3=svm_classifier.predict(Xtest3n)\n",
    "    for i in range(0,len(y_p3)):\n",
    "        dict3[d3[i]]=y_p3[i]\n",
    "    r1=evaluate(dict1,'twitter-test1.txt','svm')\n",
    "    r2=evaluate(dict2,'twitter-test2.txt','svm')\n",
    "    r3=evaluate(dict3,'twitter-test3.txt','svm')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f274b97-00eb-4f88-80c1-4560d2ce35d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  best svm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4283d724-dc7e-49e2-ae6f-fda8f8700c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# svm\n",
    "# get svm train data and test data preprocessed\n",
    "# 1-3 Ngram\n",
    "svm_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "# change to ngram vector\n",
    "X_train_svm = svm_vectorizer.fit_transform(Xtrain_1)\n",
    "# resample \n",
    "rus = RandomUnderSampler()\n",
    "Xtrain_sr, Ytrain_sr = rus.fit_resample(X_train_svm, Ytrain)\n",
    "Xtrains = normalize(Xtrain_sr, norm='l2')\n",
    "\n",
    "# ngram features for test set\n",
    "svm_ngram_feature={}\n",
    "for datasets in testsets:\n",
    "    svm_ngram_feature[datasets]=normalize(svm_vectorizer.transform(tweets[datasets]),norm='l2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "80ae79b0-b4b5-4284-88a2-fffa3a6f35f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for svm classifier on test set\n",
      "twitter-test1.txt (svm_classifier): 0.632\n",
      "            positive  negative  neutral\n",
      "positive    0.630     0.053     0.317     \n",
      "negative    0.192     0.545     0.264     \n",
      "neutral     0.218     0.102     0.679     \n",
      "\n",
      "Results for svm classifier on test set\n",
      "twitter-test2.txt (svm_classifier): 0.662\n",
      "            positive  negative  neutral\n",
      "positive    0.702     0.039     0.259     \n",
      "negative    0.190     0.523     0.287     \n",
      "neutral     0.272     0.072     0.657     \n",
      "\n",
      "Results for svm classifier on test set\n",
      "twitter-test3.txt (svm_classifier): 0.583\n",
      "            positive  negative  neutral\n",
      "positive    0.676     0.056     0.268     \n",
      "negative    0.196     0.422     0.381     \n",
      "neutral     0.301     0.096     0.604     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svm=LinearSVC()\n",
    "svm.fit(Xtrains,Ytrain_sr)\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=svm.predict(svm_ngram_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for svm classifier on test set')\n",
    "    evaluate(dict_output,item,'svm_classifier')\n",
    "    confusion(dict_output,item,'svm_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b916e66-1970-4ecb-bd1c-deb3d8b81ee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e72927-a0a0-4bed-8809-be434b2ecccd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### step 4 test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34a4bdb7-53fd-4693-bd31-20239598bdf0",
   "metadata": {},
   "source": [
    "for i in range (0,len(resampled_data)):\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(normalized_data[i][0],normalized_data[i][1] )\n",
    "    \n",
    "    print(Name_list[i])\n",
    "    dict0={}\n",
    "    y_p0=rf.predict(Xdevn)\n",
    "    for i in range(0,len(y_p0)):\n",
    "        dict0[d0[i]]=y_p0[i]\n",
    "    r0=evaluate(dict0,'twitter-dev-data.txt','nb_classifier')\n",
    "    dict1={}\n",
    "    y_p1=rf.predict(Xtest1n)\n",
    "    for i in range(0,len(y_p1)):\n",
    "        dict1[d1[i]]=y_p1[i]\n",
    "    dict2={}\n",
    "    y_p2=rf.predict(Xtest2n)\n",
    "    for i in range(0,len(y_p2)):\n",
    "        dict2[d2[i]]=y_p2[i]\n",
    "    dict3={}\n",
    "    y_p3=rf.predict(Xtest3n)\n",
    "    for i in range(0,len(y_p3)):\n",
    "        dict3[d3[i]]=y_p3[i]\n",
    "    r1=evaluate(dict1,'twitter-test1.txt','nb_classifier')\n",
    "    r2=evaluate(dict2,'twitter-test2.txt','nb_classifier')\n",
    "    r3=evaluate(dict3,'twitter-test3.txt','nb_classifier')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200bd1f-d616-47d5-a20d-98982fb5e804",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Best RF model out put slow so saved. change back to code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "20cf89af-71ec-4964-96f4-3f09ec03b8bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rf\n",
    "# get rf train data and test data preprocessed\n",
    "# 1-3 Ngram\n",
    "rf_vectorizer = CountVectorizer()\n",
    "# change to ngram vector\n",
    "X_train_rf = rf_vectorizer.fit_transform(Xtrain_1)\n",
    "# resample \n",
    "rus = RandomUnderSampler()\n",
    "Xtrain_rr, Ytrain_rr = rus.fit_resample(X_train_rf, Ytrain)\n",
    "Xtrainr = normalize(Xtrain_rr, norm='l2')\n",
    "\n",
    "# ngram features for test set\n",
    "rf_bow_feature={}\n",
    "for datasets in testsets:\n",
    "    rf_bow_feature[datasets]=normalize(rf_vectorizer.transform(tweets[datasets]),norm='l2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32954787-ba78-4baa-8485-b9c48b3d5076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a random forest classifier with 100 trees\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the random forest classifier to the training data\n",
    "rf.fit(Xtrainr, Ytrain_rr)\n",
    "\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=rf.predict(rf_bow_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for rf classifier on test set')\n",
    "    evaluate(dict_output,item,'rf')\n",
    "    confusion(dict_output,item,'rf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de3781-dfcf-4132-9145-685a063233ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "filename = 'random_forest_model.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51493719-135c-4630-9c99-f822513450dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for rf classifier on test set\n",
      "twitter-test1.txt (rf): 0.590\n",
      "            positive  negative  neutral\n",
      "positive    0.643     0.059     0.298     \n",
      "negative    0.239     0.522     0.239     \n",
      "neutral     0.262     0.111     0.628     \n",
      "\n",
      "Results for rf classifier on test set\n",
      "twitter-test2.txt (rf): 0.603\n",
      "            positive  negative  neutral\n",
      "positive    0.712     0.053     0.235     \n",
      "negative    0.208     0.480     0.312     \n",
      "neutral     0.374     0.067     0.559     \n",
      "\n",
      "Results for rf classifier on test set\n",
      "twitter-test3.txt (rf): 0.557\n",
      "            positive  negative  neutral\n",
      "positive    0.683     0.069     0.248     \n",
      "negative    0.255     0.428     0.317     \n",
      "neutral     0.298     0.101     0.601     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'random_forest_model.sav'\n",
    "# load the model from the file\n",
    "rf_loaded = pickle.load(open(filename, 'rb'))\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=rf_loaded.predict(rf_bow_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for rf classifier on test set')\n",
    "    evaluate(dict_output,item,'rf')\n",
    "    confusion(dict_output,item,'rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69432edb-3f3b-45ce-9895-69f2b8f08e8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP ( deep learning) slow and low performance so not included"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a7771d5-806e-4ee2-b748-dcda89b17891",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Train  MLP classifier\n",
    "for i in range (0,len(resampled_data)):\n",
    "    mlp=MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "    mlp.fit(normalized_data[i][0],normalized_data[i][1] )\n",
    "    \n",
    "    print(Name_list[i])\n",
    "    dict0={}\n",
    "    y_p0=rf.predict(Xdevn)\n",
    "    for i in range(0,len(y_p0)):\n",
    "        dict0[d0[i]]=y_p0[i]\n",
    "    r0=evaluate(dict0,'twitter-dev-data.txt','nb_classifier')\n",
    "    dict1={}\n",
    "    y_p1=rf.predict(Xtest1n)\n",
    "    for i in range(0,len(y_p1)):\n",
    "        dict1[d1[i]]=y_p1[i]\n",
    "    dict2={}\n",
    "    y_p2=rf.predict(Xtest2n)\n",
    "    for i in range(0,len(y_p2)):\n",
    "        dict2[d2[i]]=y_p2[i]\n",
    "    dict3={}\n",
    "    y_p3=rf.predict(Xtest3n)\n",
    "    for i in range(0,len(y_p3)):\n",
    "        dict3[d3[i]]=y_p3[i]\n",
    "    r1=evaluate(dict1,'twitter-test1.txt','nb_classifier')\n",
    "    r2=evaluate(dict2,'twitter-test2.txt','nb_classifier')\n",
    "    r3=evaluate(dict3,'twitter-test3.txt','nb_classifier')\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1fe3a0e-4486-4775-b0ad-3d6a538bafcf",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# create an instance of MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "\n",
    "# fit the model to the training data\n",
    "mlp.fit(normalized_data[3][0], normalized_data[3][1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e819a8a-7101-4c71-8c75-b30c855c0339",
   "metadata": {},
   "source": [
    "dict1={}\n",
    "y_p1=mlp.predict(Xtest1n)\n",
    "for i in range(0,len(y_p1)):\n",
    "    dict1[d1[i]]=y_p1[i]\n",
    "dict2={}\n",
    "y_p2=mlp.predict(Xtest2n)\n",
    "for i in range(0,len(y_p2)):\n",
    "    dict2[d2[i]]=y_p2[i]\n",
    "dict3={}\n",
    "y_p3=mlp.predict(Xtest3n)\n",
    "for i in range(0,len(y_p3)):\n",
    "    dict3[d3[i]]=y_p3[i]\n",
    "r1=evaluate(dict1,'twitter-test1.txt','mlp')\n",
    "r2=evaluate(dict2,'twitter-test2.txt','mlp')\n",
    "r3=evaluate(dict3,'twitter-test3.txt','mlp')\n",
    "c1=confusion(dict1,'twitter-test1.txt','mlp')\n",
    "c2=confusion(dict2,'twitter-test2.txt','mlp')\n",
    "c3=confusion(dict3,'twitter-test3.txt','mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c92838-fa4f-448f-8fd7-4a8dbffdb424",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1bece18b-ad69-4fba-b935-38cd341d0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(svc, 'svc_model.joblib')\n",
    "# The model trained by svc but not linear svc\n",
    "# load model\n",
    "svc_loaded = joblib.load('svc_model.joblib')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ad44f91-e6ef-49d1-ae20-0918f62634bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# show the SVC using SVC but not linear very slow and worse than Linear SVC\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=svc_loaded.predict(svm_ngram_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for svm classifier on test set')\n",
    "    evaluate(dict_output,item,'svm_classifier')\n",
    "    confusion(dict_output,item,'svm_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ae56dc89-0ef1-4ed6-88eb-6a6259fc7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change svc to probablity \n",
    "calibrated_svc = CalibratedClassifierCV(svm)\n",
    "# The following ensemble is too slow so I saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c2711-379b-4942-bf6f-d016843b0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_classifier = VotingClassifier(estimators=[('nb', nb_classifier),('svc1',svc_loaded),\n",
    "                                                   ('rf', rf_loaded),('svc2',calibrated_svc)],\n",
    "                                                   \n",
    "                                        voting='soft')\n",
    "\n",
    "# Train the ensemble classifier on your data\n",
    "# under sampler ngram same as features in svc\n",
    "ensemble_classifier.fit(Xtrains, Ytrain_sr)\n",
    "# over sampler svm\n",
    "#ensemble_classifier.fit(Xtrainb, Ytrain_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38ecce-1fec-4b91-8341-3d72af38457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "joblib.dump(ensemble_classifier, 'ensemble_classifier.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0684b47-d4b3-42da-99c3-01e9486887f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "54a757a7-a9f5-47a9-bf5e-e09878599c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ensemble_classifier classifier on test set\n",
      "twitter-test1.txt (ensemble_classifier): 0.631\n",
      "            positive  negative  neutral\n",
      "positive    0.633     0.059     0.308     \n",
      "negative    0.180     0.574     0.246     \n",
      "neutral     0.211     0.110     0.679     \n",
      "\n",
      "Results for ensemble_classifier classifier on test set\n",
      "twitter-test2.txt (ensemble_classifier): 0.638\n",
      "            positive  negative  neutral\n",
      "positive    0.689     0.045     0.266     \n",
      "negative    0.189     0.524     0.288     \n",
      "neutral     0.315     0.079     0.605     \n",
      "\n",
      "Results for ensemble_classifier classifier on test set\n",
      "twitter-test3.txt (ensemble_classifier): 0.583\n",
      "            positive  negative  neutral\n",
      "positive    0.670     0.061     0.268     \n",
      "negative    0.216     0.435     0.349     \n",
      "neutral     0.285     0.097     0.617     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the trained ensemble classifier from disk\n",
    "# the file is 600 mb but training requires few hours\n",
    "ensemble_classifier = joblib.load('ensemble_classifier.pkl')\n",
    "\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=ensemble_classifier.predict(svm_ngram_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for ensemble_classifier classifier on test set')\n",
    "    evaluate(dict_output,item,'ensemble_classifier')\n",
    "    confusion(dict_output,item,'ensemble_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf0376-5ae9-4ab8-a9f8-9bad95090986",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3 LSTM Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f9014a7a-5c13-4fff-a003-fe7d3c7aaaf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the embeddings into a dictionary\n",
    "embeddings_index = {}\n",
    "with open( 'glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Define the maximum number of words \n",
    "max_words = 5000\n",
    "\n",
    "# Define the embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "# Initialize an embedding matrix with zeros\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2608b13b-4645-4e80-a697-3e9e81092693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into individual words\n",
    "words = []\n",
    "for sentence in Xtrain_1:\n",
    "    words += sentence.lower().split()\n",
    "# Get the most common words and their counts\n",
    "word_counts = collections.Counter(words)\n",
    "\n",
    "# Select the top 5000 words by frequency\n",
    "top_words = [word for word, count in word_counts.most_common(4999)]\n",
    "# build word index\n",
    "word_index = {}\n",
    "for word in top_words:\n",
    "    if word not in word_index:\n",
    "        word_index[word] = len(word_index) + 1\n",
    "word_index['<PAD>'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4eb1e9a3-405c-4f1a-a9b4-606f9d3952d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have one function called preprocess_data in first part , as this is second question , so I called it preprocess_data2\n",
    "def preprocess_data2(x_train, y_train, word_index, max_len=20):\n",
    "    word_index=word_index\n",
    "   \n",
    "    X_train_processed = []\n",
    "    for sentence in x_train:\n",
    "        x = [word_index.get(word, 0) for word in sentence.split()][:max_len]\n",
    "        x += [0] * (max_len - len(x))\n",
    "        X_train_processed.append(x)\n",
    "    Y_train_processed = [0 if label == 'positive' else 1 if label == 'neutral' else 2 for label in y_train]\n",
    "    return X_train_processed, Y_train_processed\n",
    "\n",
    "\n",
    "# Train test preprocess requires balancing so different from others  Also, Shuffle= True \n",
    "batch_size=32\n",
    "X_train_processed, Y_train_processed = preprocess_data2(Xtrain_1, Ytrain, word_index)\n",
    "rus = RandomOverSampler()\n",
    "X_train_balanced, Y_train_balanced = rus.fit_resample(X_train_processed, Y_train_processed)\n",
    "# RandomUnderSampler()\n",
    "\n",
    "\n",
    "# Load and transfer data into Pytorch Tensor\n",
    "X_train_tensor = torch.tensor(X_train_balanced)\n",
    "Y_train_tensor = torch.tensor(Y_train_balanced)\n",
    "\n",
    "# build tensor data set\n",
    "dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "# Create train set loader use shuffle =True\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b301d3f5-6481-4e3f-ad08-ea16a72be7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_generater(xdata,ydata):\n",
    "    b_size=32\n",
    "    X_processed1, Y_processed1 = preprocess_data2(xdata, ydata, word_index)\n",
    "\n",
    "    # transfer data in to pytorch tensor\n",
    "    X_tensor0 = torch.tensor(X_processed1)\n",
    "    Y_tensor0 = torch.tensor(Y_processed1)\n",
    "\n",
    "    # Create \n",
    "    dataset0 = TensorDataset(X_tensor0, Y_tensor0)\n",
    "\n",
    "    # Create data loader\n",
    "\n",
    "    test_loader0 = DataLoader(dataset0, batch_size=b_size, shuffle=False)\n",
    "    return test_loader0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3994f077-5d00-4240-998e-764ca7bd0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the embedding matrix with the GloVe vectors for the words in our reference word index\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in the embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f28c5e22-3673-4119-bed5-3317ea9760f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences with 15 words: 0.5659963193720761\n",
      "Number of sentences with between 15 and 20 words: 0.40158754794793905\n",
      "Number of sentences with more than 20 words: 0.03241613267998492\n"
     ]
    }
   ],
   "source": [
    "# Define list of sentences 97% word length are lower than 20, 56% are lower than 15 , so choose 20.\n",
    "sentences = Xtrain_1\n",
    "\n",
    "# Split each sentence into a list of words and count the number of words\n",
    "num_words = [len(sentence.split()) for sentence in sentences]\n",
    "\n",
    "# Count the number of words in specific ranges\n",
    "count_15_plus = sum(1 for n in num_words if n < 15)/len(sentences)\n",
    "\n",
    "count_15_to_20 = sum(1 for n in num_words if 15 <= n <= 20)/len(sentences)\n",
    "count_20_plus = sum(1 for n in num_words if n > 20)/len(sentences)\n",
    "# Print the counts\n",
    "print(\"Number of sentences with 15 words:\", count_15_plus)\n",
    "\n",
    "print(\"Number of sentences with between 15 and 20 words:\", count_15_to_20)\n",
    "print(\"Number of sentences with more than 20 words:\", count_20_plus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231d61c-7351-472e-b3ad-8aaa1fc123b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Three models only run one and fit the relu with attention one ."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2adcc09e-e1a8-4876-bb7c-9006905ddeca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# model without attention and bilstm , lower  over all accuracy  so not used\n",
    "num_words=5000\n",
    "embedding_dim=100\n",
    "# Define the model\n",
    "class MyModel0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embedding_dim, 64,batch_first=True)\n",
    "        # low drop out at first \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Sequential(nn.Linear(64, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(32, 3),\n",
    "                                nn.ReLU())\n",
    "         \n",
    "          \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        # random initialization\n",
    "        # h0, c0 = torch.randn(1, len(x), 64), torch.randn(1, len(x), 64)\n",
    "        # zero initialization\n",
    "        h0, c0 = torch.zeros(1, len(x), 64), torch.zeros(1, len(x), 64)\n",
    "        \n",
    "        # He\n",
    "        # h0 = init.kaiming_normal_(torch.zeros(1, len(x), 64), mode='fan_in', nonlinearity='relu')\n",
    "        # c0 = init.kaiming_normal_(torch.zeros(1, len(x), 64), mode='fan_in', nonlinearity='relu')\n",
    "        output, (h0, c0) = self.lstm(embeddings, (h0, c0))\n",
    "        output = self.dropout(output)\n",
    "        output = output[:, -1, :]\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a45c2cc1-163a-42bc-bcf9-c189b050f368",
   "metadata": {},
   "source": [
    "# model with soft max and sigmoid  Used to generate final result worse than relu\n",
    "\n",
    "num_words=5000\n",
    "embedding_dim=100\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.bilstm = nn.LSTM(embedding_dim, 64, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Sequential(nn.Linear(128, 64),\n",
    "                                       nn.Tanh(),\n",
    "                                       nn.Linear(64, 1))\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Sequential(nn.Linear(128, 32),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Dropout(p=0.4),\n",
    "                                nn.Linear(32, 3),\n",
    "                                nn.Softmax(dim=1))\n",
    "         \n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        h0 = nn.init.xavier_normal_(torch.randn(2, len(x), 64))\n",
    "        c0 = nn.init.xavier_normal_(torch.randn(2, len(x), 64))\n",
    "        output, (h0, c0) = self.bilstm(embeddings, (h0, c0))\n",
    "        \n",
    "        # apply attention\n",
    "        attn_weights = self.attention(output)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(output.transpose(1, 2), attn_weights).squeeze(2)\n",
    "        \n",
    "        attn_output = self.dropout(attn_output)\n",
    "        output = self.fc(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485cbb5-178d-4792-b7fa-d17e667557b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e06893c2-feb3-461b-8b3d-5c7895c0038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model among 4 use this one and fit the saved model with this one\n",
    "\n",
    "num_words=5000\n",
    "embedding_dim=100\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, 64, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Sequential(nn.Linear(128, 64),\n",
    "                                       nn.Tanh(),\n",
    "                                       nn.Linear(64, 1))\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Sequential(nn.Linear(128, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(32, 3),\n",
    "                                nn.ReLU())\n",
    "        \n",
    "        # He initializer\n",
    "        init.kaiming_normal_(self.lstm.weight_ih_l0)\n",
    "        init.kaiming_normal_(self.lstm.weight_hh_l0)\n",
    "        init.kaiming_normal_(self.lstm.weight_ih_l0_reverse)\n",
    "        init.kaiming_normal_(self.lstm.weight_hh_l0_reverse)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        h0 = init.kaiming_normal_(torch.zeros(2, len(x), 64), mode='fan_in', nonlinearity='relu')\n",
    "        c0 = init.kaiming_normal_(torch.zeros(2, len(x), 64), mode='fan_in', nonlinearity='relu')\n",
    "        output, (h0, c0) = self.lstm(embeddings, (h0, c0))\n",
    "        \n",
    "        # apply attention\n",
    "        attn_weights = self.attention(output)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(output.transpose(1, 2), attn_weights).squeeze(2)\n",
    "        \n",
    "        attn_output = self.dropout(attn_output)\n",
    "        output = self.fc(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ec15b77-f628-4483-8410-9473608a2fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# model with relu  no attention\n",
    "\n",
    "num_words=5000\n",
    "embedding_dim=100\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embedding_dim, 64, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Sequential(nn.Linear(128, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(32, 3),\n",
    "                                nn.ReLU())\n",
    "        \n",
    "        # He initializer\n",
    "        init.kaiming_normal_(self.lstm.weight_ih_l0)\n",
    "        init.kaiming_normal_(self.lstm.weight_hh_l0)\n",
    "        init.kaiming_normal_(self.lstm.weight_ih_l0_reverse)\n",
    "        init.kaiming_normal_(self.lstm.weight_hh_l0_reverse)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = init.kaiming_normal_(torch.zeros(2, len(x), 64), mode='fan_in', nonlinearity='relu')\n",
    "        c0 = init.kaiming_normal_(torch.zeros(2, len(x), 64), mode='fan_in', nonlinearity='relu')\n",
    "        output, (h0, c0) = self.lstm(x, (h0, c0))\n",
    "        output = self.dropout(output)\n",
    "        output = output[:, -1, :]\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea4fbf4b-0dcd-4770-b301-0daac5354f39",
   "metadata": {
    "tags": []
   },
   "source": [
    "# turn to code if you want to try else use the saved data as training is time consuming.\n",
    "\n",
    "# Initialize the model\n",
    "model1 = MyModel()\n",
    "loss_list=[]\n",
    "# Define the optimizer and loss function these are all given in the question .\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model1.parameters(),lr=0.001,weight_decay=0.0007)\n",
    "# decreasing learning rate start from 0.001 and drop 20% after 5 epoch\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "# Train the model\n",
    "num_epochs = 40 # large epoch always over fit\n",
    "for epoch in range(num_epochs):\n",
    "    # decreasing learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        loss_list.append(running_loss/100)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # Print statistics\n",
    "        k=0\n",
    "        m_list=[]\n",
    "        if (i+1) % 100 == 0:\n",
    "            k+=1\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, i+1, len(X_train_balanced)//batch_size, running_loss/100))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            running_loss = 0.0\n",
    "  \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfcfe371-32fc-4ee0-9870-983d9eea8f38",
   "metadata": {},
   "source": [
    "# save trained model\n",
    "PATHa = 'lstm_01.pth'\n",
    "torch.save(model1.state_dict(), PATHa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06686f91-3e1c-40d2-9848-adb4f6c73f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the model from a file\n",
    "PATHa = 'lstm_01.pth'\n",
    "model2 = MyModel()\n",
    "model2.load_state_dict(torch.load(PATHa))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ba07dce-9358-495a-bef4-8f85cef38845",
   "metadata": {},
   "source": [
    "# Plot the loss function can not plot without trainning\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "29eebb53-e858-4dff-91cc-ab8e927809d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on train set\n",
    "def predict_model(xt,yt,di,lstm_model):\n",
    "    test_loader=loader_generater(xt,yt)\n",
    "    X_p,Y_p=preprocess_data2(xt, yt, word_index)\n",
    "    model1=lstm_model\n",
    "    model1.eval()\n",
    "    #change to test model for example drop rate will disappear.\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        y_pred = []\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model1(inputs)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            y_pred.extend(predicted.tolist())   \n",
    "            \n",
    "            \n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    y_pred_back = []\n",
    "\n",
    "    for label in y_pred:\n",
    "        if label == 0:\n",
    "            y_pred_back.append('positive')\n",
    "        elif label == 1:\n",
    "            y_pred_back.append('neutral')\n",
    "        else:\n",
    "            y_pred_back.append('negative')\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy on test set: %d %%' % accuracy)\n",
    "\n",
    "    # Get predicted values\n",
    "    \n",
    "    \n",
    "\n",
    "    # calculate F1 score I use this to compare with sem eval 2017 performance\n",
    "    f1 = f1_score(Y_p, y_pred, average='macro')\n",
    "    print('F1 score:', f1)\n",
    "\n",
    "    \n",
    "    dicti2={}\n",
    "    for i in range(0,len(y_pred_back)):\n",
    "        dicti2[di[i]]=y_pred_back[i]\n",
    "    return dicti2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c355c78-361f-47ae-8646-4b29bfd6679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 66 %\n",
      "F1 score: 0.6696740339205461\n",
      "twitter-training-data.txt (lstm): 0.705\n",
      "            positive  negative  neutral\n",
      "positive    0.693     0.018     0.289     \n",
      "negative    0.072     0.539     0.389     \n",
      "neutral     0.186     0.037     0.777     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change model2to model1 to see the new trained model.\n",
    "lstm0=predict_model(Xtrain_1,Ytrain,D,model2)\n",
    "evaluate(lstm0,'twitter-training-data.txt','lstm')\n",
    "confusion(lstm0,'twitter-training-data.txt','lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50e25ee4-69b7-4377-ba7c-77d75575f9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 61 %\n",
      "F1 score: 0.6156307286101398\n",
      "twitter-dev-data.txt (lstm): 0.647\n",
      "            positive  negative  neutral\n",
      "positive    0.648     0.037     0.316     \n",
      "negative    0.109     0.486     0.405     \n",
      "neutral     0.211     0.071     0.718     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change model2 to model1 to see the new trained model.\n",
    "lstmd=predict_model(Xdev_1,Ydev,d0,model2)\n",
    "evaluate(lstmd,'twitter-dev-data.txt','lstm')\n",
    "confusion(lstmd,'twitter-dev-data.txt','lstm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9fdb5991-828d-40f7-875d-0316fadb4d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 64 %\n",
      "F1 score: 0.6350923392676382\n",
      "Accuracy on test set: 65 %\n",
      "F1 score: 0.6183694889998699\n",
      "Accuracy on test set: 59 %\n",
      "F1 score: 0.5779451897510944\n",
      "twitter-test1.txt (lstm): 0.641\n",
      "            positive  negative  neutral\n",
      "positive    0.675     0.043     0.281     \n",
      "negative    0.197     0.517     0.287     \n",
      "neutral     0.227     0.085     0.688     \n",
      "\n",
      "twitter-test2.txt (lstm): 0.642\n",
      "            positive  negative  neutral\n",
      "positive    0.739     0.033     0.228     \n",
      "negative    0.230     0.470     0.300     \n",
      "neutral     0.339     0.058     0.603     \n",
      "\n",
      "twitter-test3.txt (lstm): 0.585\n",
      "            positive  negative  neutral\n",
      "positive    0.704     0.058     0.238     \n",
      "negative    0.199     0.392     0.409     \n",
      "neutral     0.290     0.085     0.625     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change model2 to model1 to see new trained model\n",
    "lstm1=predict_model(Xtest1_1,Ytest1,d1,model2)\n",
    "lstm2=predict_model(Xtest2_1,Ytest2,d2,model2)\n",
    "lstm3=predict_model(Xtest3_1,Ytest3,d3,model2)\n",
    "evaluate(lstm1,'twitter-test1.txt','lstm')\n",
    "confusion(lstm1,'twitter-test1.txt','lstm')\n",
    "evaluate(lstm2,'twitter-test2.txt','lstm')\n",
    "confusion(lstm2,'twitter-test2.txt','lstm')\n",
    "evaluate(lstm3,'twitter-test3.txt','lstm')\n",
    "confusion(lstm3,'twitter-test3.txt','lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583774f-ffa0-4944-b2b7-d2ac1d266e7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 4 the  final output function for all classifier(only best performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91af94-3468-437d-977f-280f077d05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "# I changed the structure of this part as i don't want to output all features even I tried them all\n",
    "# I want to only output best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fe2623f4-6c15-4402-a545-9cedf7495666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bayesian classifier on test set\n",
      "twitter-test1.txt (nb_classifier): 0.589\n",
      "            positive  negative  neutral\n",
      "positive    0.576     0.071     0.354     \n",
      "negative    0.211     0.535     0.254     \n",
      "neutral     0.202     0.140     0.658     \n",
      "\n",
      "Results for Bayesian classifier on test set\n",
      "twitter-test2.txt (nb_classifier): 0.622\n",
      "            positive  negative  neutral\n",
      "positive    0.659     0.057     0.284     \n",
      "negative    0.168     0.513     0.319     \n",
      "neutral     0.280     0.081     0.639     \n",
      "\n",
      "Results for Bayesian classifier on test set\n",
      "twitter-test3.txt (nb_classifier): 0.570\n",
      "            positive  negative  neutral\n",
      "positive    0.639     0.068     0.293     \n",
      "negative    0.225     0.419     0.356     \n",
      "neutral     0.254     0.120     0.626     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best bayesian model\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=nb_classifier.predict(nb_ngram_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for Bayesian classifier on test set')\n",
    "    evaluate(dict_output,item,'nb_classifier')\n",
    "    confusion(dict_output,item,'nb_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9b19c1c1-c014-42e5-b994-17251a3e2e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for svm classifier on test set\n",
      "twitter-test1.txt (svm_classifier): 0.636\n",
      "            positive  negative  neutral\n",
      "positive    0.640     0.047     0.313     \n",
      "negative    0.185     0.548     0.267     \n",
      "neutral     0.217     0.109     0.674     \n",
      "\n",
      "Results for svm classifier on test set\n",
      "twitter-test2.txt (svm_classifier): 0.648\n",
      "            positive  negative  neutral\n",
      "positive    0.697     0.042     0.261     \n",
      "negative    0.204     0.512     0.283     \n",
      "neutral     0.315     0.064     0.621     \n",
      "\n",
      "Results for svm classifier on test set\n",
      "twitter-test3.txt (svm_classifier): 0.589\n",
      "            positive  negative  neutral\n",
      "positive    0.680     0.062     0.258     \n",
      "negative    0.204     0.436     0.360     \n",
      "neutral     0.287     0.090     0.623     \n",
      "\n",
      "Results for svm classifier on test set\n",
      "twitter-test4.txt (svm_classifier): 0.589\n",
      "            positive  negative  neutral\n",
      "positive    0.680     0.062     0.258     \n",
      "negative    0.204     0.436     0.360     \n",
      "neutral     0.287     0.090     0.623     \n",
      "\n",
      "Results for svm classifier on test set\n",
      "twitter-test5.txt (svm_classifier): 0.648\n",
      "            positive  negative  neutral\n",
      "positive    0.697     0.042     0.261     \n",
      "negative    0.204     0.512     0.283     \n",
      "neutral     0.315     0.064     0.621     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best svm model ( saved model) \n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=svm.predict(svm_ngram_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for svm classifier on test set')\n",
    "    evaluate(dict_output,item,'svm_classifier')\n",
    "    confusion(dict_output,item,'svm_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e1e744c1-f770-4654-ab2c-56e69ea96d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for rf classifier on test set\n",
      "twitter-test1.txt (rf): 0.590\n",
      "            positive  negative  neutral\n",
      "positive    0.643     0.059     0.298     \n",
      "negative    0.239     0.522     0.239     \n",
      "neutral     0.262     0.111     0.628     \n",
      "\n",
      "Results for rf classifier on test set\n",
      "twitter-test2.txt (rf): 0.603\n",
      "            positive  negative  neutral\n",
      "positive    0.712     0.053     0.235     \n",
      "negative    0.208     0.480     0.312     \n",
      "neutral     0.374     0.067     0.559     \n",
      "\n",
      "Results for rf classifier on test set\n",
      "twitter-test3.txt (rf): 0.557\n",
      "            positive  negative  neutral\n",
      "positive    0.683     0.069     0.248     \n",
      "negative    0.255     0.428     0.317     \n",
      "neutral     0.298     0.101     0.601     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best random forest  saved model\n",
    "# To generate new model Best RF model out put slow so saved. change back to code to run\n",
    "filename = 'random_forest_model.sav'\n",
    "# load the model from the file\n",
    "rf_loaded = pickle.load(open(filename, 'rb'))\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=rf_loaded.predict(rf_bow_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for rf classifier on test set')\n",
    "    evaluate(dict_output,item,'rf')\n",
    "    confusion(dict_output,item,'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "27bc0962-2333-46b3-bd61-fadcf3863e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ensemble_classifier classifier on test set\n",
      "twitter-test1.txt (ensemble_classifier): 0.631\n",
      "            positive  negative  neutral\n",
      "positive    0.633     0.059     0.308     \n",
      "negative    0.180     0.574     0.246     \n",
      "neutral     0.211     0.110     0.679     \n",
      "\n",
      "Results for ensemble_classifier classifier on test set\n",
      "twitter-test2.txt (ensemble_classifier): 0.638\n",
      "            positive  negative  neutral\n",
      "positive    0.689     0.045     0.266     \n",
      "negative    0.189     0.524     0.288     \n",
      "neutral     0.315     0.079     0.605     \n",
      "\n",
      "Results for ensemble_classifier classifier on test set\n",
      "twitter-test3.txt (ensemble_classifier): 0.583\n",
      "            positive  negative  neutral\n",
      "positive    0.670     0.061     0.268     \n",
      "negative    0.216     0.435     0.349     \n",
      "neutral     0.285     0.097     0.617     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ensemble model saved model\n",
    "# to run the model run all in Ensemble\n",
    "# load the trained ensemble classifier from disk\n",
    "ensemble_classifier = joblib.load('ensemble_classifier.pkl')\n",
    "\n",
    "for item in testsets:\n",
    "    dict_output={}\n",
    "    \n",
    "    y_predict=ensemble_classifier.predict(svm_ngram_feature[item])\n",
    "    for i in range(0,len(y_predict)):\n",
    "        dict_output[tweetids[item][i]]=y_predict[i]\n",
    "    print('Results for ensemble_classifier classifier on test set')\n",
    "    evaluate(dict_output,item,'ensemble_classifier')\n",
    "    confusion(dict_output,item,'ensemble_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6f285e1-7262-418d-8e9a-998c9569d64f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 67 %\n",
      "F1 score: 0.6711897102223879\n",
      "twitter-training-data.txt (lstm): 0.707\n",
      "            positive  negative  neutral\n",
      "positive    0.693     0.018     0.289     \n",
      "negative    0.071     0.541     0.388     \n",
      "neutral     0.186     0.037     0.777     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best lstm\n",
    "# Load the model from a file change model2 to model1 for new trained model\n",
    "\n",
    "# train model fit to check whether over fit\n",
    "lstm0=predict_model(Xtrain_1,Ytrain,D,model2)\n",
    "evaluate(lstm0,'twitter-training-data.txt','lstm')\n",
    "confusion(lstm0,'twitter-training-data.txt','lstm')\n",
    "# use all test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d5254607-1db2-42ea-901d-70dba744e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 64 %\n",
      "F1 score: 0.6366140959853323\n",
      "Results for lstm classifier on test set\n",
      "twitter-test1.txt (lstm): 0.642\n",
      "            positive  negative  neutral\n",
      "positive    0.681     0.043     0.275     \n",
      "negative    0.196     0.509     0.295     \n",
      "neutral     0.217     0.088     0.695     \n",
      "\n",
      "Accuracy on test set: 66 %\n",
      "F1 score: 0.6277978090958684\n",
      "Results for lstm classifier on test set\n",
      "twitter-test2.txt (lstm): 0.652\n",
      "            positive  negative  neutral\n",
      "positive    0.748     0.029     0.223     \n",
      "negative    0.220     0.481     0.300     \n",
      "neutral     0.330     0.060     0.610     \n",
      "\n",
      "Accuracy on test set: 59 %\n",
      "F1 score: 0.5778602053147516\n",
      "Results for lstm classifier on test set\n",
      "twitter-test3.txt (lstm): 0.583\n",
      "            positive  negative  neutral\n",
      "positive    0.706     0.059     0.235     \n",
      "negative    0.204     0.389     0.407     \n",
      "neutral     0.287     0.087     0.626     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LSTM model change model2 to model1 for new trained model\n",
    "for item in testsets:\n",
    "    y_predict={}\n",
    "    y_predict=predict_model(tweets[item],tweetgts[item],tweetids[item],model2)\n",
    "    print('Results for lstm classifier on test set')\n",
    "    evaluate(y_predict,item,'lstm')\n",
    "    confusion(y_predict,item,'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26218950-6213-4650-9b9a-03ed25f7472c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
